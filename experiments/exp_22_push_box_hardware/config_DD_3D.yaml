dataset:
  state_dim: 18 # pusher position-3D (3) + 5 descriptor keypoints (3D)
  action_dim: 2 # pusher velocity-2D
  data_augmentation:
    enabled: False
    type: "homogeneous_transform"
    pos_min: [-0.2,-0.2,0]
    pos_max: [0.2, 0.2, 0]
    yaw_min: -0.5
    yaw_max: 0.5
  action_function:
    type: "spartan_ee_setpoint_linear_velocity_2D"
  observation_function:
    type: "spartan_ee_points"
    ee_points:
      - [0.0, 0., 0.]
  visual_observation_function:
    type: "precomputed_descriptor_keypoints_3D"
    camera_name: "d415_01"
  visual_observation:
    enabled: False
  object_state_shape: [5,3]
  robot_state_shape: [1,3]
  downsample_rate: 1 # downsample rate, original data is 5Hz

dynamics_net:
  model_type: "mlp"

train:
  random_seed: 1
  n_epoch: 1200
  lr: 1e-4
  adam_beta1: 0.9
  batch_size: 128
  batch_norm: False
  nf_hidden: 500
  num_workers: 20
  train_valid_ratio: 0.9
  valid_frequency: 5 # validate after this many training epochs
  log_per_iter: 50
  ckp_per_iter: 10000
  resume_epoch: -1
  resume_iter: -1
  n_history: 1
  n_rollout: 5
#  lr_scheduler:
#    type: "ReduceLROnPlateau"
#    patience: 10 # epochs
#    factor: 0.1 # reduce LR by this factor
#    threshold: 1e-4
#    threshold_mode: "rel" # can be ["rel", "abs"]
#    cooldown: 5
#    min_lr: 1e-8
  lr_scheduler:
    type: "StepLR"
    enabled: True
    step_size: 700 # epochs
    gamma: 0.1 # reduce LR by this factor

  valid_loss_type: 'l2_final_step'

loss_function:
  mse_final_step:
    enabled: False
    weight: 1.0

  mse:
    enabled: True
    weight: 1.0

  smooth_l1:
    enabled: False
    weight: 1.0

  smooth_l1_final_step:
    enabled: False
    weight: 1.0

  weight_matrix_sparsity:
    enabled: False
    weight: 0.0001



mpc:
  mpc_dy_epoch: -1
  mpc_dy_iter: -1
  num_episodes: 10
  num_timesteps: 35
  optim_type: mppi      # mppi/cem/gd
#  n_look_ahead: 10
  n_look_ahead: 10
  use_fixed_mpc_horizon: False
  add_noise_to_observation: False # whether to add noise to observation during MPC rollouts

  # pass into computing reward
  reward:
#    terminal_cost_only: True
#    goal_type: "FINAL_STATE" # "TRAJECTORY"
    terminal_cost_only: True
    goal_type: "TRAJECTORY"
    p: 2 # cost norm
    normalize: True



  hardware:
    mpc_horizon:
#      type: "PLAN_LENGTH"
#      type: "MIN_HORIZON"
#      H_min: 5
      type: "FIXED"
#      H_fixed: 7
      H_fixed: 10
      H_init: 10
    action_extension_type: "CONSTANT" # or  "ZERO"

    goal_reward_threshold: -0.0005
#    reward_improvement_tol: 0.002 # maybe should be smaller . . .
    reward_improvement_tol: 0.0015 # maybe should be smaller . . .
    terminate_if_no_improvement: True # terminate if below the improvement threshold




  mppi: # model predictive path integral
    beta_filter: 0.7
#    reward_weight: 20. # used in 'optimize_action' function call
    reward_weight: 100 #
    action_sampling:
      sigma: 0.05
      noise_type: 'normal'
      add_samples_around_random_shooting_trajectory: True
    n_sample: 1000
    action_lower_lim: [-0.3, -0.3] # large enough not to bind
    action_upper_lim: [0.3, 0.3]
    n_update_iter_init: 3   # optimization steps for the first update ~ planning
    n_update_iter: 3
    angle_min_deg: 0
    angle_max_deg: 360
    angle_step_deg: 2
    vel_min: 0.02
    vel_max: 0.25
    vel_step: 0.05

  random_shooting:
    angle_min_deg: 0
    angle_max_deg: 360
    angle_step_deg: 1
    vel_min: 0.02
    vel_max: 0.25
    vel_step: 0.01
